{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "project-final-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQkqSCcn_KCy"
      },
      "source": [
        "#This is the notebook for the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWkyAG-Q_FO9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import optim\n",
        "from torch import Tensor\n",
        "import torchtext\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "\n",
        "from typing import List, Tuple\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "#plt.switch_backend('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4k3jGbWofj4",
        "outputId": "e5710a26-cec0-462a-bd90-216f550ceb25"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmbyomnHiUfr"
      },
      "source": [
        "# load and format data\n",
        "def load_data(data):\n",
        "    colnames = [\"In\",\"Out\"]\n",
        "    data = pd.read_csv(data,sep=\"OUT:\",header = None,names=colnames, engine='python')\n",
        "    data['In'] = data['In'].apply(lambda x: re.sub('IN:','', str(x)))\n",
        "    return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwgZzK1AbsAl"
      },
      "source": [
        "# data path for Experiment 1\n",
        "data_path = \"/content/drive/MyDrive/SCAN/simple_split/\"\n",
        "\n",
        "# data path for Experiment 2\n",
        "# data_path = \"/content/drive/MyDrive/SCAN/length_split/\"\n",
        "\n",
        "\n",
        "# data path for Experiment 3\n",
        "# data_path = \"/content/drive/MyDrive/SCAN/add_prim_split/\"\n",
        "\n",
        "\n",
        "# filenames of the training and test data for Experiment 1\n",
        "train_filename = \"tasks_train_simple\"\n",
        "test_filename = \"tasks_test_simple\"\n",
        "\n",
        "# filenames of the training and test data for Experiment 2\n",
        "train_filename = \"tasks_train_length\"\n",
        "test_filename = \"tasks_test_length\"\n",
        "\n",
        "# filenames of the training and test data for Experiment 3 turn left\n",
        "train_filename = \"tasks_train_addprim_turn_left\"\n",
        "test_filename = \"tasks_test_addprim_turn_left\"\n",
        "\n",
        "# filenames of the training and test data for Experiment 3 turn left\n",
        "train_filename = \"tasks_train_addprim_jump\"\n",
        "test_filename = \"tasks_test_addprim_jump\"\n",
        "\n",
        "\n",
        "train_data = load_data(data_path + train_filename + \".txt\")\n",
        "test_data = load_data(data_path + test_filename + \".txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhRFfTPf7G7r"
      },
      "source": [
        "train_csv = train_filename + \".csv\"\n",
        "test_csv = test_filename + \".csv\"\n",
        "\n",
        "train_to_tsv = train_data.to_csv(data_path + train_csv, index=False)\n",
        "test_to_tsv = test_data.to_csv(data_path + test_csv, index=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWstSKH_bwxF"
      },
      "source": [
        "command = Field(sequential=True, use_vocab=True, init_token='<SOS>', eos_token = '<EOS>')\n",
        "action = Field(sequential=True, use_vocab=True, init_token='<SOS>', eos_token = '<EOS>')\n",
        "fields = {\"In\":(\"i\", command),\"Out\":(\"o\", action)}\n",
        "data_train, data_test = TabularDataset.splits(path=data_path, train=train_csv, test=test_csv, format=\"csv\", fields=fields)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQDdQFjX7SL1"
      },
      "source": [
        "command.build_vocab(data_train)\n",
        "action.build_vocab(data_train)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPTI8HnviGpL"
      },
      "source": [
        "batch_size = 1\n",
        "train_iterator, test_iterator = BucketIterator.splits((data_train, data_test), sort_key=lambda x: len(x.i), batch_size=batch_size, device=device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6vlZUFyjXIw"
      },
      "source": [
        "def command_to_sentence(sequence):\n",
        "  out = []\n",
        "  for i in sequence:\n",
        "    out.append(command.vocab.itos[i])\n",
        "  return out\n",
        "  \n",
        "def action_to_sentence(sequence):\n",
        "  out = []\n",
        "  for i in sequence:\n",
        "    out.append(action.vocab.itos[i])\n",
        "  return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8WrOXcfjrfE"
      },
      "source": [
        "src_vocab_size = len(command.vocab.itos)\n",
        "trg_vocab_size = len(action.vocab.itos)\n",
        "src_pad_idx = command.vocab.stoi['<pad>']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OP7-v7PkQrU"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim, src_vocab_size, trg_vocab_size, src_pad_idx, n_heads, n_encoder_layers, n_decoder_layers, dim_forward, dropout_p, max_len, device):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.device = device\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, emb_dim)\n",
        "        self.src_pos = nn.Embedding(max_len, emb_dim)\n",
        "        self.trg_emb = nn.Embedding(trg_vocab_size, emb_dim)\n",
        "        self.trg_pos = nn.Embedding(max_len, emb_dim)\n",
        "        self.transformer = nn.Transformer(emb_dim, n_heads, n_encoder_layers, n_decoder_layers, dim_forward, dropout_p)\n",
        "        self.fc = nn.Linear(emb_dim, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "    \n",
        "    def generate_src_padding_mask(self, src):\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "        return src_mask.to(self.device)\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        src_seq_length, N = src.shape\n",
        "        trg_seq_length, N = trg.shape\n",
        "        src_positions = (torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(self.device))\n",
        "        trg_positions = (torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).to(self.device))\n",
        "        emb_src = self.dropout(self.src_emb(src)+self.src_pos(src_positions))\n",
        "        emb_trg = self.dropout(self.trg_emb(trg)+self.trg_pos(trg_positions))\n",
        "        src_mask = self.generate_src_padding_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
        "        \n",
        "        output = self.transformer(emb_src, emb_trg, tgt_mask=trg_mask)#, src_key_padding_mask = src_mask\n",
        "        output = self.fc(output)\n",
        "        return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZr42K9VNmDP"
      },
      "source": [
        "# hyperparameters for Experiment 1 and 2\n",
        "emb_dim = 200\n",
        "n_heads = 8\n",
        "n_encoder_layers = 2\n",
        "n_decoder_layers = 2\n",
        "dim_forward = 480\n",
        "dropout_p = 0\n",
        "max_len=100\n",
        "lr = 0.0001\n",
        "\n",
        "# hyperparameters for Experiment 3\n",
        "emb_dim = 240\n",
        "#n_heads = 8\n",
        "#n_encoder_layers = 2\n",
        "#n_decoder_layers = 2\n",
        "#dim_forward = 480\n",
        "#dropout_p = 0\n",
        "#max_len=100\n",
        "#lr = 0.0001\n",
        "\n",
        "model = TransformerModel(emb_dim, src_vocab_size, trg_vocab_size, src_pad_idx, n_heads, n_encoder_layers, n_decoder_layers, dim_forward, dropout_p, max_len, device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD4IDCGApBJW"
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "clip = 5\n",
        "\n",
        "def trainIters(model, n_iters, print_every=1000, plot_every=100):\n",
        "\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    for i in range(1, n_iters + 1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        training_pair = next(iter(train_iterator))\n",
        "        input_tensor = training_pair.i.long().to(device)\n",
        "        target_tensor = training_pair.o.long().to(device)\n",
        "\n",
        "        # forward\n",
        "        pred = model(input_tensor, target_tensor[:-1, :])\n",
        "        pred = pred.reshape(-1, pred.shape[2])\n",
        "        # remove the start token\n",
        "        target_tensor = target_tensor[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(pred, target_tensor)\n",
        "\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, i / n_iters), i, i / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if i % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uwcRC1Qh37P"
      },
      "source": [
        "def evaluate(model, input_tensor, max_length = 50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = []\n",
        "        outputs = [action.vocab.stoi[\"<SOS>\"]]\n",
        "        \n",
        "        for i in range(max_length):\n",
        "            target_tensor =  torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "            output = model(input_tensor, target_tensor)\n",
        "\n",
        "            best_guess = output.argmax(2)[-1, :].item()\n",
        "            outputs.append(best_guess)\n",
        "\n",
        "            if best_guess == action.vocab.stoi[\"<EOS>\"]:\n",
        "                break\n",
        "    return outputs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qEBONb4jCc4"
      },
      "source": [
        "# start training\n",
        "trainIters(model, 50000, print_every=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtvhEBj8q5M"
      },
      "source": [
        "# get training accuracy\n",
        "total = {}\n",
        "\n",
        "counts = {}\n",
        "\n",
        "print('Testing on ' + str(len(train_iterator)) + ' examples')\n",
        "for batch_idx, batch in enumerate(train_iterator):\n",
        "    l = len(batch.o)\n",
        "    pred_out = evaluate(model, batch.i.long().to(device), max_length = 50)\n",
        "    if (str(batch.o.squeeze(1).tolist()) == str(pred_out)):\n",
        "        total[l] = total[l] +1 if l in total else 1\n",
        "    counts[l] = counts[l] +1 if l in counts else 1\n",
        "a = pd.Series(total)\n",
        "b = pd.Series(counts)\n",
        "accuracy = a.sum()/b.sum()\n",
        "print('Training accuracy: ' + str(accuracy))\n",
        "plt.bar(b.keys(), a/b)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tooFpYrosTCK"
      },
      "source": [
        "# get test accuracy\n",
        "\n",
        "total = {}\n",
        "\n",
        "counts = {}\n",
        "\n",
        "print('Testing on ' + str(len(test_iterator)) + ' examples')\n",
        "for batch_idx, batch in enumerate(test_iterator):\n",
        "    l = len(batch.o)\n",
        "    pred_out = evaluate(model, batch.i.long().to(device), max_length = 50)\n",
        "    if (str(batch.o.squeeze(1).tolist()) == str(pred_out)):\n",
        "        total[l] = total[l] +1 if l in total else 1\n",
        "    counts[l] = counts[l] +1 if l in counts else 1\n",
        "a = pd.Series(total)\n",
        "b = pd.Series(counts)\n",
        "accuracy = a.sum()/b.sum()\n",
        "print('correct items.', a.sum())\n",
        "print('all items', b.sum())\n",
        "print('Test  accuracy: ' + str(accuracy))\n",
        "plt.bar(b.keys(), a/b)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}