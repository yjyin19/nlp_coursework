{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NWkyAG-Q_FO9"
   },
   "outputs": [],
   "source": [
    "## The implementation of the Tranformer model is adapt from \n",
    "## https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/seq2seq_transformer/seq2seq_transformer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "import torchtext\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4k3jGbWofj4",
    "outputId": "e5710a26-cec0-462a-bd90-216f550ceb25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmbyomnHiUfr"
   },
   "outputs": [],
   "source": [
    "# load and format data\n",
    "def load_data(data):\n",
    "    colnames = [\"In\",\"Out\"]\n",
    "    data = pd.read_csv(data,sep=\"OUT:\",header = None,names=colnames, engine='python')\n",
    "    data['In'] = data['In'].apply(lambda x: re.sub('IN:','', str(x)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwgZzK1AbsAl"
   },
   "outputs": [],
   "source": [
    "# data path for Experiment 1\n",
    "data_path = \"/content/drive/MyDrive/SCAN/simple_split/\"\n",
    "\n",
    "# data path for Experiment 2\n",
    "# data_path = \"/content/drive/MyDrive/SCAN/length_split/\"\n",
    "\n",
    "\n",
    "# data path for Experiment 3\n",
    "# data_path = \"/content/drive/MyDrive/SCAN/add_prim_split/\"\n",
    "\n",
    "\n",
    "# filenames of the training and test data for Experiment 1\n",
    "train_filename = \"tasks_train_simple\"\n",
    "test_filename = \"tasks_test_simple\"\n",
    "\n",
    "# filenames of the training and test data for Experiment 2\n",
    "train_filename = \"tasks_train_length\"\n",
    "test_filename = \"tasks_test_length\"\n",
    "\n",
    "# filenames of the training and test data for Experiment 3 turn left\n",
    "train_filename = \"tasks_train_addprim_turn_left\"\n",
    "test_filename = \"tasks_test_addprim_turn_left\"\n",
    "\n",
    "# filenames of the training and test data for Experiment 3 turn left\n",
    "train_filename = \"tasks_train_addprim_jump\"\n",
    "test_filename = \"tasks_test_addprim_jump\"\n",
    "\n",
    "\n",
    "train_data = load_data(data_path + train_filename + \".txt\")\n",
    "test_data = load_data(data_path + test_filename + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WhRFfTPf7G7r"
   },
   "outputs": [],
   "source": [
    "train_csv = train_filename + \".csv\"\n",
    "test_csv = test_filename + \".csv\"\n",
    "\n",
    "train_to_tsv = train_data.to_csv(data_path + train_csv, index=False)\n",
    "test_to_tsv = test_data.to_csv(data_path + test_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jWstSKH_bwxF"
   },
   "outputs": [],
   "source": [
    "command = Field(sequential=True, use_vocab=True, init_token='<SOS>', eos_token = '<EOS>')\n",
    "action = Field(sequential=True, use_vocab=True, init_token='<SOS>', eos_token = '<EOS>')\n",
    "fields = {\"In\":(\"i\", command),\"Out\":(\"o\", action)}\n",
    "data_train, data_test = TabularDataset.splits(path=data_path, train=train_csv, test=test_csv, format=\"csv\", fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PQDdQFjX7SL1"
   },
   "outputs": [],
   "source": [
    "command.build_vocab(data_train)\n",
    "action.build_vocab(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MPTI8HnviGpL"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_iterator, test_iterator = BucketIterator.splits((data_train, data_test), sort_key=lambda x: len(x.i), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V6vlZUFyjXIw"
   },
   "outputs": [],
   "source": [
    "def command_to_sentence(sequence):\n",
    "    out = []\n",
    "    for i in sequence:\n",
    "        out.append(command.vocab.itos[i])\n",
    "    return out\n",
    "  \n",
    "def action_to_sentence(sequence):\n",
    "    out = []\n",
    "    for i in sequence:\n",
    "        out.append(action.vocab.itos[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O8WrOXcfjrfE"
   },
   "outputs": [],
   "source": [
    "src_vocab_size = len(command.vocab.itos)\n",
    "trg_vocab_size = len(action.vocab.itos)\n",
    "src_pad_idx = command.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5OP7-v7PkQrU"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, src_vocab_size, trg_vocab_size, src_pad_idx, n_heads, n_encoder_layers, n_decoder_layers, dim_forward, dropout_p, max_len, device):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, emb_dim)\n",
    "        self.src_pos = nn.Embedding(max_len, emb_dim)\n",
    "        self.trg_emb = nn.Embedding(trg_vocab_size, emb_dim)\n",
    "        self.trg_pos = nn.Embedding(max_len, emb_dim)\n",
    "        self.transformer = nn.Transformer(emb_dim, n_heads, n_encoder_layers, n_decoder_layers, dim_forward, dropout_p)\n",
    "        self.fc = nn.Linear(emb_dim, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "    \n",
    "    def generate_src_padding_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        src_positions = (torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(self.device))\n",
    "        trg_positions = (torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).to(self.device))\n",
    "        emb_src = self.dropout(self.src_emb(src)+self.src_pos(src_positions))\n",
    "        emb_trg = self.dropout(self.trg_emb(trg)+self.trg_pos(trg_positions))\n",
    "        src_mask = self.generate_src_padding_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
    "        \n",
    "        output = self.transformer(emb_src, emb_trg, tgt_mask=trg_mask)#, src_key_padding_mask = src_mask\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HZr42K9VNmDP"
   },
   "outputs": [],
   "source": [
    "# hyperparameters for Experiment 1 and 2\n",
    "emb_dim = 200\n",
    "n_heads = 8\n",
    "n_encoder_layers = 2\n",
    "n_decoder_layers = 2\n",
    "dim_forward = 480\n",
    "dropout_p = 0\n",
    "max_len=100\n",
    "lr = 0.0001\n",
    "\n",
    "# hyperparameters for Experiment 3\n",
    "emb_dim = 240\n",
    "#n_heads = 8\n",
    "#n_encoder_layers = 2\n",
    "#n_decoder_layers = 2\n",
    "#dim_forward = 480\n",
    "#dropout_p = 0\n",
    "#max_len=100\n",
    "#lr = 0.0001\n",
    "\n",
    "model = TransformerModel(emb_dim, src_vocab_size, trg_vocab_size, src_pad_idx, n_heads, n_encoder_layers, n_decoder_layers, dim_forward, dropout_p, max_len, device).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BD4IDCGApBJW"
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "clip = 5\n",
    "\n",
    "def trainIters(model, n_iters, print_every=1000, plot_every=100):\n",
    "\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    for i in range(1, n_iters + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        training_pair = next(iter(train_iterator))\n",
    "        input_tensor = training_pair.i.long().to(device)\n",
    "        target_tensor = training_pair.o.long().to(device)\n",
    "\n",
    "        # forward\n",
    "        pred = model(input_tensor, target_tensor[:-1, :])\n",
    "        pred = pred.reshape(-1, pred.shape[2])\n",
    "        # remove the start token\n",
    "        target_tensor = target_tensor[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(pred, target_tensor)\n",
    "\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, i / n_iters), i, i / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0uwcRC1Qh37P"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, input_tensor, max_length = 50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        outputs = [action.vocab.stoi[\"<SOS>\"]]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            target_tensor =  torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "            output = model(input_tensor, target_tensor)\n",
    "\n",
    "            best_guess = output.argmax(2)[-1, :].item()\n",
    "            outputs.append(best_guess)\n",
    "\n",
    "            if best_guess == action.vocab.stoi[\"<EOS>\"]:\n",
    "                break\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qEBONb4jCc4"
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "trainIters(model, 50000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARtvhEBj8q5M"
   },
   "outputs": [],
   "source": [
    "# get training accuracy\n",
    "total = {}\n",
    "\n",
    "counts = {}\n",
    "\n",
    "print('Testing on ' + str(len(train_iterator)) + ' examples')\n",
    "for batch_idx, batch in enumerate(train_iterator):\n",
    "    l = len(batch.o)\n",
    "    pred_out = evaluate(model, batch.i.long().to(device), max_length = 50)\n",
    "    if (str(batch.o.squeeze(1).tolist()) == str(pred_out)):\n",
    "        total[l] = total[l] +1 if l in total else 1\n",
    "    counts[l] = counts[l] +1 if l in counts else 1\n",
    "a = pd.Series(total)\n",
    "b = pd.Series(counts)\n",
    "accuracy = a.sum()/b.sum()\n",
    "print('Training accuracy: ' + str(accuracy))\n",
    "plt.bar(b.keys(), a/b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tooFpYrosTCK"
   },
   "outputs": [],
   "source": [
    "# get test accuracy\n",
    "\n",
    "total = {}\n",
    "\n",
    "counts = {}\n",
    "\n",
    "print('Testing on ' + str(len(test_iterator)) + ' examples')\n",
    "for batch_idx, batch in enumerate(test_iterator):\n",
    "    l = len(batch.o)\n",
    "    pred_out = evaluate(model, batch.i.long().to(device), max_length = 50)\n",
    "    if (str(batch.o.squeeze(1).tolist()) == str(pred_out)):\n",
    "        total[l] = total[l] +1 if l in total else 1\n",
    "    counts[l] = counts[l] +1 if l in counts else 1\n",
    "a = pd.Series(total)\n",
    "b = pd.Series(counts)\n",
    "accuracy = a.sum()/b.sum()\n",
    "print('correct items.', a.sum())\n",
    "print('all items', b.sum())\n",
    "print('Test  accuracy: ' + str(accuracy))\n",
    "plt.bar(b.keys(), a/b)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "project-final-transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
